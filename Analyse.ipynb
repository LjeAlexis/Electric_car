{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install spacy\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!pip install sentencepiece\n",
        "!pip install scipy\n",
        "!pip install transformers torch\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "blt-XuNRZ9Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers tensorflow keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqprsvx7rZ2B",
        "outputId": "8424a7a5-541b-4011-a7fa-a9cb6169e145"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import time\n",
        "import logging\n",
        "from dateutil.parser import parse\n",
        "import re\n",
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "import torch\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "6scGs_uNaIq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCH8dBVFMk2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJgGaEmfMkzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LEMMATISATION"
      ],
      "metadata": {
        "id": "3-vVzMiXN5lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('output.csv', sep=';')"
      ],
      "metadata": {
        "id": "p5tVn7xPuvrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('fr_core_news_sm')"
      ],
      "metadata": {
        "id": "yDeartlDvIg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_lemmatize(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Lemmatisation et nettoyage\n",
        "    tokens = [token.lemma_.lower() for token in doc if token.pos_ not in ['DET', 'ADP', 'ADV'] and not token.is_stop and token.lemma_.isalpha()]\n",
        "\n",
        "    # Retourner le texte nettoyé et lemmatisé\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "yaQfAllnvIe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_cleaned'] = df['text'].apply(clean_and_lemmatize)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UUzXyAXpvIb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data_cara_lem.csv')\n"
      ],
      "metadata": {
        "id": "BvEBHxDOE34X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSE DE SENTIMENT"
      ],
      "metadata": {
        "id": "72k8ww9HOA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data_cara_lem.csv')"
      ],
      "metadata": {
        "id": "IrDVezfOMkl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "uwyRpydcMLGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def analyze_sentiment_batch(texts, batch_size=200):\n",
        "    model.eval()  # Mettre le modèle en mode évaluation\n",
        "    weighted_sentiment_scores = []  # Liste pour stocker les scores pondérés\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing Sentiments\"):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)  # Obtient les probabilités pour chaque classe\n",
        "\n",
        "            # Calcul des scores pondérés\n",
        "            # Supposons que les étiquettes sont numérotées de 1 à 5\n",
        "            stars = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32).to(device)\n",
        "            weighted_scores = (probs * stars).sum(dim=1)  # Score pondéré pour chaque prédiction\n",
        "            weighted_sentiment_scores.extend(weighted_scores.cpu().numpy())\n",
        "\n",
        "    return weighted_sentiment_scores"
      ],
      "metadata": {
        "id": "tonCix-d-k3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].astype(str)\n",
        "sentiment_scores = analyze_sentiment_batch(df['text'].tolist())\n",
        "\n",
        "# Ajouter les scores de sentiment au DataFrame\n",
        "df['sentiment_score'] = sentiment_scores\n",
        "\n",
        "df.to_csv('data_cara_wsent.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dClXfVMC-k1P",
        "outputId": "23bf4b89-5ba6-4461-8738-04d44163db0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Sentiments: 100%|██████████| 449/449 [47:59<00:00,  6.41s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data_cara_wsent.csv', sep=',')"
      ],
      "metadata": {
        "id": "c3dXiIsmsb1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## MAIN VISUALISATION\n",
        "\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
        "\n",
        "df.sort_values('date', inplace=True)\n",
        "\n",
        "df_first_period = df[df['date'] <= '2016-12-31']\n",
        "df_second_period = df[df['date'] > '2016-12-31']\n",
        "\n",
        "df_first_period['sentiment_smoothed'] = df_first_period['sentiment_score'].rolling(window=50, center=True).mean()\n",
        "df_second_period['sentiment_smoothed'] = df_second_period['sentiment_score'].rolling(window=50, center=True).mean()\n",
        "\n",
        "window_size = 51\n",
        "polynomial_order = 3\n",
        "\n",
        "df_first_period['sentiment_smoothed_sg'] = savgol_filter(df_first_period['sentiment_smoothed'].fillna(df_first_period['sentiment_score']), window_size, polynomial_order)\n",
        "df_second_period['sentiment_smoothed_sg'] = savgol_filter(df_second_period['sentiment_smoothed'].fillna(df_second_period['sentiment_score']), window_size, polynomial_order)\n",
        "\n",
        "plt.figure(figsize=(14, 14))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(df_first_period['date'], df_first_period['sentiment_score'], alpha=0.3, label='Original Sentiment Score')\n",
        "plt.plot(df_first_period['date'], df_first_period['sentiment_smoothed'], color='darkorange', label='Moyenne mobile')\n",
        "plt.title('Analyse de sentiment pre 2016')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(df_second_period['date'], df_second_period['sentiment_score'], alpha=0.3, label='Original Sentiment Score')\n",
        "plt.plot(df_second_period['date'], df_second_period['sentiment_smoothed'], color='darkorange', label='Moyenne mobile')\n",
        "plt.title('Analyse de sentiment post 2017')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S0W8_S9u-kzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### FONCTION POUR SENTIMENT PAR TOPIC DE FORUM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "df['text_cleaned'] = df['text_cleaned'].dropna().astype(str)\n",
        "texts = df['text_cleaned'].tolist()\n",
        "themes = df['theme'].tolist()\n",
        "\n",
        "def analyze_sentiment_batch_by_theme(texts, themes, batch_size=200):\n",
        "    model.eval()\n",
        "    sentiment_scores_by_theme = {}\n",
        "\n",
        "    for theme in set(themes):\n",
        "        theme_texts = [text for text, th in zip(texts, themes) if th == theme]\n",
        "\n",
        "        if not theme_texts:\n",
        "            print(f\"No texts to process for theme: {theme}\")\n",
        "            continue\n",
        "\n",
        "        weighted_sentiment_scores = []\n",
        "\n",
        "        for i in tqdm(range(0, len(theme_texts), batch_size), desc=f\"Analyzing Sentiments for theme: {theme}\"):\n",
        "            batch_texts = theme_texts[i:i + batch_size]\n",
        "\n",
        "\n",
        "            if not batch_texts:\n",
        "                print(f\"No more texts to process in batch starting at index {i} for theme: {theme}\")\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = torch.softmax(outputs.logits, dim=1)\n",
        "                stars = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32).to(device)\n",
        "                weighted_scores = (probs * stars).sum(dim=1)\n",
        "                weighted_sentiment_scores.extend(weighted_scores.cpu().numpy())\n",
        "\n",
        "        sentiment_scores_by_theme[theme] = weighted_sentiment_scores\n",
        "\n",
        "    return sentiment_scores_by_theme\n",
        "\n"
      ],
      "metadata": {
        "id": "SYqJMwJdWMaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DISPERSION DES SCORES DE SENTIMENT"
      ],
      "metadata": {
        "id": "750SjqtZbKt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrer par thème spécifique, par exemple 'Tesla'\n",
        "theme_specific = 'Tesla'\n",
        "df_filtered = df[df['Theme'] == theme_specific]\n",
        "\n",
        "word_sentiment_scores = defaultdict(list)\n",
        "global_sentiment_scores = []\n",
        "\n",
        "for _, row in df_filtered.iterrows():\n",
        "    words = str(row['text_cleaned']).split()\n",
        "    sentiment_score = row['sentiment_score']\n",
        "    global_sentiment_scores.append(sentiment_score)\n",
        "    for word in words:\n",
        "        word_sentiment_scores[word].append(sentiment_score)\n",
        "\n",
        "# Préparation des données pour la visualisation\n",
        "words_to_analyze = ['prix','autonomie', 'recharge', 'batterie']\n",
        "data_for_visualization = []\n",
        "\n",
        "for word in words_to_analyze:\n",
        "    scores = word_sentiment_scores.get(word, [])\n",
        "    if scores:\n",
        "        for score in scores:\n",
        "            data_for_visualization.append({'Word': word, 'Sentiment Score': score})\n",
        "\n",
        "# Conversion en DataFrame pour la visualisation\n",
        "df_viz = pd.DataFrame(data_for_visualization)\n",
        "\n",
        "# Affichage des statistiques globales pour le thème\n",
        "if global_sentiment_scores:\n",
        "    print(f\"Statistiques globales pour le thème '{theme_specific}':\")\n",
        "    print(f\"Moyenne: {np.mean(global_sentiment_scores)}\")\n",
        "    print(f\"Écart-type: {np.std(global_sentiment_scores)}\")\n",
        "    print(f\"Nombre d'occurrences: {len(global_sentiment_scores)}\\n\")\n",
        "else:\n",
        "    print(f\"Aucune donnée pour calculer les statistiques globales du thème '{theme_specific}'.\")\n",
        "\n",
        "# Affichage des statistiques et visualisation avec des violin plots\n",
        "if not df_viz.empty:\n",
        "    sns.violinplot(x=\"Word\", y=\"Sentiment Score\", data=df_viz)\n",
        "    plt.title(f'Distribution des scores de sentiment pour les mots sélectionnés dans le thème {theme_specific}')\n",
        "    plt.show()\n",
        "\n",
        "    for word in words_to_analyze:\n",
        "        scores = np.array(word_sentiment_scores.get(word, []))\n",
        "        if scores.size > 0:\n",
        "            print(f\"Statistiques pour le mot '{word}' dans le thème {theme_specific}:\")\n",
        "            print(f\"Moyenne: {np.mean(scores)}\")\n",
        "            print(f\"Écart-type: {np.std(scores)}\")\n",
        "            print(f\"Nombre d'occurrences: {len(scores)}\\n\")\n",
        "else:\n",
        "    print(\"Aucune donnée pour la visualisation dans le thème spécifié.\")"
      ],
      "metadata": {
        "id": "haiIhqnYbOqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTROPIE"
      ],
      "metadata": {
        "id": "rjr4CjnnOJfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy(words):\n",
        "    word_counts = Counter(words)\n",
        "    total_words = sum(word_counts.values())\n",
        "    entropy = -sum((count/total_words) * math.log2(count/total_words) for count in word_counts.values())\n",
        "    return entropy\n",
        "\n",
        "# Fonction pour extraire les mots et calculer l'entropie pour chaque groupe\n",
        "def entropy_by_theme_date(df):\n",
        "    # Groupe par theme et date\n",
        "    grouped = df.groupby(['theme', 'date'])\n",
        "\n",
        "    # Calculer l'entropie pour chaque groupe\n",
        "    entropy_results = grouped['text_cleaned'].apply(lambda texts: calculate_entropy(' '.join(texts).split()))\n",
        "\n",
        "    return entropy_results.reset_index(name='entropy')\n",
        "\n",
        "# Appliquer la fonction à votre DataFrame\n",
        "entropy_df = entropy_by_theme_date(df)"
      ],
      "metadata": {
        "id": "-Tnf80ydOMg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculer la moyenne mobile de l'entropie\n",
        "window_size = 5  # La taille de la fenêtre peut être ajustée selon les besoins\n",
        "rolling_entropies = entropy_df.set_index('date').groupby('theme')['entropy'].rolling(window=window_size, center=True, min_periods=1).mean().reset_index()\n",
        "\n",
        "# Visualiser les entropies lissées\n",
        "plt.figure(figsize=(20, 12))\n",
        "for theme, group in rolling_entropies.groupby('theme'):\n",
        "    plt.plot(group['date'], group['entropy'], label=theme)\n",
        "\n",
        "plt.title(f\"Entropie hebdomadaire des mots - lissage avec moyenne mobile sur une fenêtre de {window_size}\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Entropie moyenne\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(title='Theme')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "0sQYthfDOMVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}