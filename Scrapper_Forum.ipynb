{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eogoFAh5cmxh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install selenium\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2rhg0z0crL7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "driver = webdriver.Chrome(options=chrome_options)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = \"output.csv\"\n",
        "\n",
        "csvFile = open(path_to_file, 'a', encoding=\"utf-8\")\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "all_rows = []\n",
        "\n",
        "all_urls = [  \"https://forum-exemple.com/topic/qsdfg\",\n",
        "              \"https://forum-exemple.com/topic/dfskjndnk\"\n",
        "]\n",
        "\n",
        "for url in all_urls:\n",
        "    try:\n",
        "        driver.get(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    while True:\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        print(driver.current_url)\n",
        "\n",
        "        comment_elements = soup.find_all('balise', class_='balise')\n",
        "\n",
        "        for comment_element in comment_elements:\n",
        "\n",
        "            h3_element = comment_element.select_one('balise')\n",
        "            if h3_element:\n",
        "                pseudo_text = h3_element.get_text(strip=True)\n",
        "            else:\n",
        "                a_element = comment_element.find('a', class_='balise')\n",
        "                if a_element:\n",
        "                    pseudo_text = a_element.get_text(strip=True)\n",
        "                else:\n",
        "                    strong_element = comment_element.find('balise')\n",
        "                    if strong_element:\n",
        "                        pseudo_text = strong_element.get_text(strip=True)\n",
        "                    else:\n",
        "                        pseudo_text = \"\"\n",
        "\n",
        "\n",
        "            date_element = comment_element.find('time')\n",
        "            date_text = date_element['datetime'] if date_element else \"\"\n",
        "\n",
        "            # Extraction du contenu du commentaire\n",
        "            content_element = comment_element.find('div', class_='balise')\n",
        "            sub_element_text = '\\n'.join([p.get_text(strip=True) for p in content_element.find_all('p')]) if content_element else \"\"\n",
        "\n",
        "            # Ajout des données extraites à la liste\n",
        "            all_rows.append([url, pseudo_text, date_text, sub_element_text])\n",
        "\n",
        "        # Navigation vers la page suivante\n",
        "        try:\n",
        "          next_page = driver.find_element(By.XPATH, \"balise\")\n",
        "          a_element = next_page.find_element(By.TAG_NAME, 'a')\n",
        "          href_value = a_element.get_attribute('href')\n",
        "          href_value = href_value.replace('#comments', '')\n",
        "          driver.get(href_value)\n",
        "          time.sleep(3)\n",
        "        except NoSuchElementException:\n",
        "          print(\"No more pages for\", url)\n",
        "          break\n",
        "\n",
        "# Écriture des données dans le fichier CSV\n",
        "    for row in all_rows:\n",
        "      csvWriter.writerow(row)\n",
        "\n",
        "# Fermeture du fichier CSV et du WebDriver\n",
        "csvFile.close()\n",
        "driver.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxrRlEi-gEka",
        "outputId": "f98e9afd-9387-463a-8868-32680a25982d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [

          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhKvDTHZc6t4"
      },
      "outputs": [],
      "source": [
        "#csvFile.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
